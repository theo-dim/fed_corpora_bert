{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"7_FOMC_Analysis_Sentence.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lHecaO_7JG6L"},"source":["# F. Analyse sentiment by sentence\n","## Import necessary libraries"]},{"cell_type":"code","metadata":{"id":"X2MZzFxZw5pK"},"source":["import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","IN_COLAB"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdwYN7SIw5pd"},"source":["if IN_COLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4OqZfqcw5pZ"},"source":["# Check Device\n","if IN_COLAB:\n","  torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZowjtkbmbV0"},"source":["# Use TPU\n","if IN_COLAB:\n","  # TPU Setting\n","  import os\n","  assert os.environ['COLAB_TPU_ADDR'], 'Select TPU: Runtime > Change runtime type > Hardware accelerator'  \n","  VERSION = \"20200220\"\n","  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","  !python pytorch-xla-env-setup.py --version $VERSION"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skbUZfszq9WD"},"source":["# Uninstall existing versions:\n","!pip uninstall numpy -y\n","!pip uninstall pandas -y\n","!pip uninstall tqdm -y\n","!pip uninstall torch -y\n","!pip uninstall scikit-plot -y\n","!pip uninstall transformers -y\n","\n","# Install packages:\n","!pip install numpy==1.16.4\n","!pip install pandas==0.25.0\n","!pip install tqdm==4.43.0\n","!pip install torch==1.4.0\n","!pip install scikit-plot\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYiY1suXzjKg"},"source":["# Python libraries\n","import pprint\n","pprint.pprint(sys.path)\n","import datetime as dt\n","import re\n","import pickle\n","from tqdm.notebook import tqdm\n","import os\n","import sys\n","import time\n","import logging\n","import random\n","from collections import defaultdict, Counter\n","\n","# Data Science modules\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","plt.style.use('ggplot')\n","\n","# Import Scikit-learn models\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import model_selection\n","from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, StratifiedKFold, learning_curve, RandomizedSearchCV\n","import scikitplot as skplt\n","\n","# Import nltk modules and download dataset\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","stop = set(stopwords.words('english'))\n","\n","# Import Pytorch modules\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","from torch.autograd import Variable\n","from torch.optim import Adam, AdamW"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-37Bpnkw5pT"},"source":["# Set logger\n","logger = logging.getLogger('mylogger')\n","logger.setLevel(logging.INFO)\n","\n","timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n","formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n","\n","fh = logging.FileHandler('log_model.txt')\n","fh.setLevel(logging.DEBUG)\n","fh.setFormatter(formatter)\n","logger.addHandler(fh)\n","\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.INFO)\n","ch.setFormatter(formatter)\n","logger.addHandler(ch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYf9oi6fw5pV"},"source":["# Set Random Seed\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","rand_seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cb55v8tkw5pX"},"source":["# Set Seaborn Style\n","sns.set(style='white', context='notebook', palette='deep')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSVVc9rew5pd"},"source":["## Load and process input data"]},{"cell_type":"code","metadata":{"id":"sLFBUS8Hw5pf"},"source":["if IN_COLAB:\n","  employment_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/Employment/'\n","  cpi_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/CPI/'\n","  fed_rates_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/FEDRates/'\n","  fx_rates_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/FXRates/'\n","  gdp_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/GDP/'\n","  ism_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/ISM/'\n","  sales_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/Sales/'\n","  treasury_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/Treasury/'\n","  fomc_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/FOMC/'\n","  preprocessed_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/preprocessed/'\n","  train_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/train_data/'\n","  output_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/result/'\n","  keyword_lm_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/LoughranMcDonald/'\n","  glove_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/GloVe/'\n","  model_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/models/'\n","else:\n","  employment_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/Employment/'\n","  cpi_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/CPI/'\n","  fed_rates_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/FEDRates/'\n","  fx_rates_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/FXRates/'\n","  gdp_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/GDP/'\n","  ism_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/ISM/'\n","  sales_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/Sales/'\n","  treasury_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/Treasury/'\n","  fomc_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/FOMC/'\n","  preprocessed_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/preprocessed/'\n","  train_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/train_data/'\n","  output_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/result/'\n","  keyword_lm_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/LoughranMcDonald/'\n","  glove_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/GloVe/'\n","  model_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TC_3RwSGw5ph","scrolled":true},"source":["# Load previously processed non-text data\n","# Load data\n","#file = open(train_dir + 'nontext_train_small.csv', 'rb')\n","#meta_df = pickle.load(file)\n","#file.close()\n","meta_df = pd.read_csv (train_dir + 'nontext_train_small.csv')\n","print(meta_df.shape)\n","meta_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wW8pWlJjw5pi"},"source":["# List of Non-text columns\n","meta_columns = meta_df.drop(columns=['target']).columns.tolist()\n","meta_columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UgJAxAcw5pk"},"source":["# Load text data\n","file = open(preprocessed_dir + 'text_no_split.pickle', 'rb') # Original text\n","text_no_split = pickle.load(file)\n","file.close()\n","\n","file = open(preprocessed_dir + 'text_keyword.pickle', 'rb') # Paragraphs filtered for those having keywords\n","text_keyword = pickle.load(file)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vw7FJtqEw5pm"},"source":["# Check the number of records per document type\n","def plot_num_rec_word(df):    \n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,5))\n","    sns.countplot(x='type', data=df, ax=ax1)\n","    ax1.set_title('The number of records (original)', fontsize=16)\n","    ax1.tick_params('x', labelrotation=10)\n","    sns.barplot(data=df, x='type', y='word_count', ax=ax2)\n","    ax2.set_title('The number of words (original)', fontsize=16)\n","    ax2.tick_params('x', labelrotation=10)\n","    \n","plot_num_rec_word(text_no_split)\n","text_no_split.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3QzdeKTghXx"},"source":["plot_num_rec_word(text_keyword)\n","text_keyword.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyOa-NLLw5pr"},"source":["# # Drop meeting script data\n","# text_no_split = text_no_split.loc[text_no_split['type'] != 'meeting_script']\n","# text_split_200 = text_split_200.loc[text_split_200['type'] != 'meeting_script']\n","# text_keyword = text_keyword.loc[text_keyword['type'] != 'meeting_script']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4lNeHd26w5pw"},"source":["### Select text dataframe"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"3zAOMqcEghXx"},"source":["def combine_meeting_script(df):\n","    new_df = df.loc[df['type'] != 'meeting_script']\n","    print(new_df.shape)\n","    meeting_script_df = df.loc[df['type'] == 'meeting_script'].reset_index(drop=True)\n","    data_list = []\n","    for i, row in tqdm(meeting_script_df.iterrows(), total=meeting_script_df.shape[0]):\n","        if i == 0:\n","            prev_row = row\n","            continue\n","        if prev_row['date'].strftime('%Y-%m-%d') == row['date'].strftime('%Y-%m-%d'):\n","            prev_row['text'] = prev_row['text'] + row['text']\n","        else:\n","            prev_row['speaker'] = \"\"\n","            prev_row['word_count'] = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', prev_row['text']))\n","            data_list.append(list(prev_row))\n","            prev_row = row\n","            \n","    prev_row['speaker'] = \"\"\n","    prev_row['word_count'] = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', prev_row['text']))\n","    data_list.append(list(prev_row))\n","    new_meeting_script_df = pd.DataFrame(data_list, columns=df.columns)\n","    new_df = pd.concat([new_df, new_meeting_script_df], axis=0)\n","    print(new_df.shape)\n","    return new_df\n","\n","train_df = combine_meeting_script(text_no_split)\n","plot_num_rec_word(train_df)\n","train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AeyWwu_w5p0"},"source":["# Check balance of Rate Decision\n","g = sns.FacetGrid(train_df, col='type', height=6, aspect=0.5)\n","g.map(sns.countplot, 'next_decision')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3MOs7yZKdFz","scrolled":false},"source":["# Check distribution\n","def plot_distribution(df, kde):\n","    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15,10))\n","    doc_type = 'statement'\n","    sns.distplot(df.loc[df['type'] == doc_type]['word_count'], bins=20, ax=ax1, kde=kde, color='r')\n","    ax1.set_title(doc_type)\n","    doc_type = 'minutes'\n","    sns.distplot(df.loc[df['type'] == doc_type]['word_count'], bins=20, ax=ax2, kde=kde, color='y')\n","    ax2.set_title(doc_type)\n","    doc_type = 'presconf_script'\n","    sns.distplot(df.loc[df['type'] == doc_type]['word_count'], bins=20, ax=ax3, kde=kde, color='g')\n","    ax3.set_title(doc_type)\n","    doc_type = 'meeting_script'\n","    sns.distplot(df.loc[df['type'] == doc_type]['word_count'], bins=20, ax=ax4, kde=kde, color='b')\n","    ax4.set_title(doc_type)\n","    doc_type = 'speech'\n","    sns.distplot(df.loc[df['type'] == doc_type]['word_count'], bins=20, ax=ax5, kde=kde, color='purple')\n","    ax5.set_title(doc_type)\n","    doc_type = 'testimony'\n","    sns.distplot(df.loc[df['type'] == doc_type]['word_count'], bins=20, ax=ax6, kde=kde, color='orange')\n","    ax6.set_title(doc_type)\n","\n","    fig.tight_layout(pad=1.0)\n","    plt.show()\n","\n","plot_distribution(train_df, False)\n","train_df.tail()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhZCs1PSghXx"},"source":["# Add a key for aggregation\n","train_df['key'] = train_df.index.get_level_values(0)\n","train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwLFvxYYghXy"},"source":["## Split each text by sentence"]},{"cell_type":"code","metadata":{"id":"dUrLFUtoghXy"},"source":["# Split the data by sentence\n","def split_df(df, text_column='text'):\n","    '''\n","    Returns a dataframe which is an extension of an input dataframe.\n","    Each row in the new dataframe has less than $split_len words in 'text'.\n","    '''\n","    split_data_list = []\n","\n","    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","        #print(\"Original Word Count: \", row['word_count'])\n","        text_list = sent_tokenize(row[text_column])\n","        for text in text_list:\n","            row['text'] = text\n","            row['word_count'] = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text))\n","            split_data_list.append(list(row))\n","            \n","    split_df = pd.DataFrame(split_data_list, columns=df.columns)\n","    split_df['decision'] = split_df['decision'].astype('Int8')\n","    split_df['next_decision'] = split_df['next_decision'].astype('Int8')\n","\n","    return split_df\n","\n","train_sent_df = split_df(train_df)\n","print(train_sent_df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51dFXr6ighXy"},"source":["# Check distribution\n","plot_distribution(train_sent_df, False)\n","train_sent_df.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwfmxAOZghXy"},"source":["## Load the trained model"]},{"cell_type":"code","metadata":{"id":"MgfAZOCkghXy"},"source":["# Load the model\n","from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n","model.load_state_dict(torch.load(model_dir + 'finphrase_model_fold_3.dict', map_location=torch.device('cpu')))\n","model.eval()\n","model.to(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_k0saR4KghXy"},"source":["## Stream the text input"]},{"cell_type":"code","metadata":{"id":"LIo3KIWAghXy"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def bert_encoder(text, max_len=48):\n","    \"\"\" Return embedded text vector as a list in max_len with a mask list\"\"\"\n","    text_token = tokenizer.tokenize(text)\n","    text_token = text_token[:max_len-2]\n","    text_token = [\"[CLS]\"] + text_token + [\"[SEP]\"]\n","    text_ids = tokenizer.convert_tokens_to_ids(text_token)\n","    text_ids += [0] * (max_len - len(text_token))\n","    pad_masks = [1] * len(text_token) + [0] * (max_len - len(text_token))\n","    segment_ids = [0] * len(text_token) + [0] * (max_len - len(text_token))\n","    \n","    return text_ids, pad_masks, segment_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AoH6cPeOghXy"},"source":["def predict(text, model):     \n","    text_ids, pad_masks, segment_ids = bert_encoder(text)\n","    \n","    text_ids = torch.tensor(text_ids).unsqueeze(0)\n","    pad_masks = torch.tensor(pad_masks).unsqueeze(0)\n","    segment_ids = torch.tensor(segment_ids).unsqueeze(0)\n","\n","    outputs = model(text_ids, pad_masks, segment_ids)[0].detach()    \n","    pred = F.softmax(outputs, dim=1).cpu().numpy()\n","    \n","    return pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f8ceR7nghXy"},"source":["# Check\n","text = \"Google is working on self driving cars, I'm bullish on $goog\"\n","predict(text, model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B53rhQw8ghXy"},"source":["def score_text(df, text_column, model):\n","    class_names = ['Negative', 'Neutral', 'Positive']\n","    scores = []\n","    predicts = []\n","    \n","    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","        score = predict(row[text_column], model)\n","        scores.append((score[0]))\n","        predicts.append(class_names[int(np.argmax(score))])\n","\n","    df['score'] = scores\n","    df['prediction'] = predicts\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RMKdbH7w5rH"},"source":["## Predict"]},{"cell_type":"code","metadata":{"id":"BwMOqGpighXy"},"source":["doc_types = train_sent_df['type'].unique().tolist()\n","doc_types"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"OzEKSRbighXy"},"source":["result_df = pd.DataFrame([], columns=train_df.columns)\n","for doc_type in doc_types:\n","    print('Processing {}...'.format(doc_type))\n","    train_sample = train_sent_df.loc[(train_sent_df['type']==doc_type)]\n","    res = score_text(train_sample, 'text', model)\n","    result_df = pd.concat([result_df, res], axis=0)\n","    \n","result_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLtvALo9ghXy"},"source":["result_df = pd.concat([result_df, pd.get_dummies(result_df['prediction'])], axis=1)\n","result_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"IIKZOM2SghXy"},"source":["result_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02rO0BZBghXy"},"source":["train_result = train_df.copy()\n","train_result['key2'] = train_result['type'] + \"_\" + train_result['key'].map(str)\n","result_df['key2'] = result_df['type'] + \"_\" + result_df['key'].map(str)\n","\n","neg = result_df.groupby(by=['key2'])['Negative'].sum()\n","neu = result_df.groupby(by=['key2'])['Neutral'].sum()\n","pos = result_df.groupby(by=['key2'])['Positive'].sum()\n","\n","train_result = train_result.merge(neg, on='key2', how='left')\n","train_result = train_result.merge(neu, on='key2', how='left')\n","train_result = train_result.merge(pos, on='key2', how='left')\n","\n","train_result['sentiment'] = (train_result['Positive'] - train_result['Negative'])\n","train_result.sort_values(['type', 'next_meeting'], inplace=True)\n","\n","print(\"The number of records: \", train_result.shape[0])\n","print(\"Sentiment is null: \", train_result['sentiment'].isnull().sum())\n","train_result.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m19Lis7FghXy"},"source":["def get_doctype_df(df, doc_type):\n","    if doc_type == 'all':\n","        new_df = df\n","    else:\n","        new_df = df.loc[df['type']==doc_type]\n","    new_df = new_df.groupby(by=['next_meeting', 'next_decision', 'next_rate'])['Negative', 'Neutral', 'Positive', 'sentiment'].sum()\n","    new_df.reset_index(level=[1,2], inplace=True)\n","    new_df.sort_index(inplace=True)\n","    new_df['next_rate_change'] = new_df['next_rate'] - new_df['next_rate'].shift(1)\n","    new_df['sentiment_pct'] = new_df['sentiment'] / (new_df['Positive'] + new_df['Negative'])\n","    new_df['sentiment_chg'] = new_df['sentiment'] - new_df['sentiment'].shift(1)\n","    return new_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ov3oezFWghXy"},"source":["stmt_df = get_doctype_df(train_result, 'statement')\n","minutes_df = get_doctype_df(train_result, 'minutes')\n","presconf_df = get_doctype_df(train_result, 'presconf_script')\n","m_script_df = get_doctype_df(train_result, 'meeting_script')\n","speech_df = get_doctype_df(train_result, 'speech')\n","testimony_df = get_doctype_df(train_result, 'testimony')\n","all_df = get_doctype_df(pd.concat([stmt_df, minutes_df, presconf_df, m_script_df, speech_df, testimony_df], axis=0), 'all')\n","all_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1x-RT-z-ghXz"},"source":["# Show corelations to target\n","def align_yaxis(ax1, v1, ax2, v2):\n","    \"\"\"adjust ax2 ylimit so that v2 in ax2 is aligned to v1 in ax1\"\"\"\n","    _, y1 = ax1.transData.transform((0, v1))\n","    _, y2 = ax2.transData.transform((0, v2))\n","    inv = ax2.transData.inverted()\n","    _, dy = inv.transform((0, 0)) - inv.transform((0, y1-y2))\n","    miny, maxy = ax2.get_ylim()\n","    ax2.set_ylim(miny+dy, maxy+dy)\n","    \n","def plot_sentiment(df, doc_type, sent_column):\n","    fig, (ax1, ax2)  = plt.subplots(2, 1, figsize=(15,10), gridspec_kw={'height_ratios': [3, 1]})\n","    sns.lineplot(y=df[sent_column], x=df.index.get_level_values(0), ax=ax1)\n","    ax1.set_xlim('1992-01-01', '2020-06-01')\n","    ax1.set_ylabel('Sentiment: {}'.format(sent_column))\n","    ax1.set_xlabel('Year')\n","    ax1.fill_between(df.index.get_level_values(0), 0, df[sent_column], where=df[sent_column] > 0, facecolor='blue', alpha=0.2)\n","    ax1.fill_between(df.index.get_level_values(0), 0, df[sent_column], where=df[sent_column] < 0, facecolor='red', alpha=0.2)\n","    ax12 = ax1.twinx()\n","    sns.lineplot(y=df['next_rate'].fillna(0), x=df.index.get_level_values(0), ax=ax12, color='green')\n","    #ax12.yaxis.grid(True, which='major')\n","    ax12.set_ylim(-4,8)\n","    ax12.legend(\"Rate\", loc='upper right')\n","    align_yaxis(ax1, 0, ax12, 0)\n","    arrow_style = dict(facecolor='black', shrink=0.05)\n","    ax12.annotate('QE1', xy=('2008-11-25', -2.5), xytext=('2008-11-25', -3.5), size=12, ha='right', arrowprops=arrow_style)\n","    ax12.annotate('QE1+', xy=('2009-03-18', -2), xytext=('2009-03-18', -3), size=12, ha='center', arrowprops=arrow_style)\n","    ax12.annotate('QE2', xy=('2010-11-03', 0), xytext=('2010-11-03', -1), size=12, ha='center', arrowprops=arrow_style)\n","    ax12.annotate('QE2+', xy=('2011-09-21', -2), xytext=('2011-09-21', -3), size=12, ha='center', arrowprops=arrow_style)\n","    ax12.annotate('QE3', xy=('2012-09-13', -2), xytext=('2012-09-13', -3), size=12, ha='center', arrowprops=arrow_style)\n","    ax12.annotate('Tapering', xy=('2013-12-18', 0), xytext=('2013-12-18', -2), size=12, ha='center', arrowprops=arrow_style)\n","\n","    \n","    plt.title('Net sentiment over years with the next FED target rate', fontsize=16)\n","\n","    corr_columns = ['sentiment', 'sentiment_pct', 'sentiment_chg', 'next_decision', 'next_rate_change', 'next_rate']\n","    sns.heatmap(df[corr_columns].astype(float).corr().iloc[:3], annot=True, fmt=\".2f\", ax=ax2, cmap= 'coolwarm', center=0, vmin=-1, vmax=1)\n","    ax2.set_title(\"Correlation\", fontsize=16)\n","    \n","    plt.suptitle(\"Doc Type: {}\".format(doc_type), fontsize=16)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYuWla-VghXz"},"source":["plot_sentiment(all_df, 'All', 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4di8ZYYghXz"},"source":["plot_sentiment(stmt_df, 'Statement', 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmMk5Zf9ghXz"},"source":["plot_sentiment(minutes_df, 'Minutes', 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRW0g3tgghXz"},"source":["plot_sentiment(presconf_df, 'Press Conference Script', 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ny7sLGUvghXz"},"source":["plot_sentiment(m_script_df, 'Meeting Script', 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-A55gNFTghXz"},"source":["plot_sentiment(speech_df, 'Speech', 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZnVyP0mgghXz"},"source":["plot_sentiment(testimony_df, 'Testimony', 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EdEn3xMNw5sI"},"source":["## Save Data"]},{"cell_type":"code","metadata":{"id":"mqwUOnPtw5sJ"},"source":["def save_data(df, file_name, dir_name=train_dir):\n","    if not os.path.exists(dir_name):\n","        os.mkdir(dir_name)\n","        \n","    # Save results to a picke file\n","    file = open(dir_name + file_name + '.pickle', 'wb')\n","    pickle.dump(df, file)\n","    file.close()\n","\n","    # Save results to a csv file\n","    df.to_csv(dir_name + file_name + '.csv', index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lynjDKNCw5sJ"},"source":["# Save text data\n","save_data(result_df, 'fomc_sent_result_df')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSawHYO3w5sM"},"source":["save_data(all_df, 'fomc_sentiment_bert_all')\n","save_data(stmt_df, 'fomc_sentiment_bert_stmt')\n","save_data(minutes_df, 'fomc_sentiment_bert_stmt')\n","save_data(presconf_df, 'fomc_sentiment_bert_stmt')\n","save_data(m_script_df, 'fomc_sentiment_bert_stmt')\n","save_data(speech_df, 'fomc_sentiment_bert_stmt')\n","save_data(testimony_df, 'fomc_sentiment_bert_stmt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9eA5E18VghXz"},"source":["## Model and Train"]},{"cell_type":"code","metadata":{"id":"wJ1PHzMbghXz"},"source":["# Load data\n","file = open(train_dir + 'train_df.pickle', 'rb')\n","train_df = pickle.load(file)\n","file.close()\n","print(train_df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAxK4cu0ghXz"},"source":["train_df.drop(columns=['statement', 'minutes', 'presconf_script', 'speech', 'testimony', 'text',\n","                      'tokenized', 'token_ids', 'tokenized_text', 'tfidf_Negative',\n","                       'tfidf_Positive', 'tfidf_Uncertainty', 'tfidf_Litigious',\n","                       'tfidf_StrongModal', 'tfidf_Constraining'], inplace=True)\n","\n","#train_df.drop(columns=['tone','cos_sim_Negative',\n","#       'cos_sim_Positive', 'cos_sim_Uncertainty', 'cos_sim_Litigious',\n","#       'cos_sim_StrongModal', 'cos_sim_Constraining'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFSPcmMnghXz"},"source":["train_df = train_df.merge(stmt_df[['Negative', 'Neutral', 'Positive', 'sentiment', 'sentiment_chg']], left_index=True, right_index=True, how='left')\n","train_df = train_df.merge(minutes_df[['Negative', 'Neutral', 'Positive', 'sentiment', 'sentiment_chg']], left_index=True, right_index=True, how='left', suffixes=('_stmt', '_minutes'))\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAH813FRghXz"},"source":["from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix\n","from sklearn.pipeline import Pipeline\n","\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, StratifiedKFold, learning_curve, RandomizedSearchCV, RepeatedStratifiedKFold\n","from sklearn.impute import SimpleImputer\n","\n","import scikitplot as skplt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yC_DlQrAghXz"},"source":["# Chck the imputation\n","data = train_df.values\n","Xtmp, y1 = np.array(data[:, 1:], dtype=np.float64), np.array(data[:, 0], dtype=np.int64)+1\n","print('The number of training data: %d' % len(Xtmp))\n","print('Missing: %d' % sum(np.isnan(Xtmp).flatten()))\n","imputer = SimpleImputer(strategy='mean')\n","imputer.fit(Xtmp)\n","X1 = imputer.transform(Xtmp)\n","print('\\nImputed:')\n","print('The number of training data: %d' % len(X1))\n","print('Missing: %d' % sum(np.isnan(X1).flatten()))\n","\n","data2 = train_df.dropna().values\n","X2, y2 = np.array(data2[:, 1:], dtype=np.float64), np.array(data2[:, 0], dtype=np.int64)+1\n","print('\\nDropped:')\n","print('The number of training data: %d' % len(X2))\n","print('Missing: %d' % sum(np.isnan(X2).flatten()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdDjnVL2ghXz"},"source":["# Chck the pipeline with imputation\n","model = RandomForestClassifier()\n","imputer = SimpleImputer(strategy='mean')\n","pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n","\n","# Random Forest\n","scores = cross_val_score(pipeline, Xtmp, y1, scoring='accuracy', cv=cv, n_jobs=-1)\n","print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n","scores = cross_val_score(pipeline, Xtmp, y1, scoring='f1_macro', cv=cv, n_jobs=-1)\n","print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_njWJxKaghXz"},"source":["random_state = 42\n","#kfold = StratifiedKFold(n_splits=10, random_state=random_state)\n","kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n","imputer = SimpleImputer(strategy='mean')\n","classifiers = []\n","classifiers.append((\"SVC\", SVC(random_state=random_state)))\n","classifiers.append((\"DecisionTree\", DecisionTreeClassifier(random_state=random_state)))\n","classifiers.append((\"AdaBoost\", AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1)))\n","classifiers.append((\"RandomForest\", RandomForestClassifier(random_state=random_state)))\n","classifiers.append((\"ExtraTrees\", ExtraTreesClassifier(random_state=random_state)))\n","classifiers.append((\"GradientBoosting\", GradientBoostingClassifier(random_state=random_state)))\n","classifiers.append((\"MultipleLayerPerceptron\", MLPClassifier(random_state=random_state)))\n","classifiers.append((\"KNeighboors\", KNeighborsClassifier(n_neighbors=3)))\n","classifiers.append((\"LogisticRegression\", LogisticRegression(random_state = random_state)))\n","classifiers.append((\"LinearDiscriminantAnalysis\", LinearDiscriminantAnalysis()))\n","classifiers.append((\"GaussianNB\", GaussianNB()))\n","classifiers.append((\"Perceptron\", Perceptron()))\n","classifiers.append((\"LinearSVC\", LinearSVC()))\n","classifiers.append((\"SGD\", SGDClassifier()))\n","\n","cv_results = []\n","classifier_name = []\n","for classifier in classifiers :\n","    cv_results.append(cross_validate(classifier[1], X1, y1, scoring=[\"accuracy\", \"f1_macro\"], cv=kfold, n_jobs=-1))\n","    classifier_name.append(classifier[0])\n","\n","cv_acc_means = []\n","cv_acc_std = []\n","cv_f1_means = []\n","cv_f1_std = []\n","for cv_result in cv_results:\n","    cv_acc_means.append(cv_result['test_accuracy'].mean())\n","    cv_acc_std.append(cv_result['test_accuracy'].std())\n","    cv_f1_means.append(cv_result['test_f1_macro'].mean())\n","    cv_f1_std.append(cv_result['test_f1_macro'].std())\n","\n","cv_res = pd.DataFrame({\"Algorithm\": classifier_name,\n","                       \"CVAccMeans\":cv_acc_means,\n","                       \"CVAccErrors\": cv_acc_std,\n","                       \"CVf1Means\":cv_f1_means,\n","                       \"CVf1Errors\": cv_f1_std}).sort_values(by='CVAccMeans', ascending=False)\n","\n","cv_res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHZiOQGughXz"},"source":["fig, ax = plt.subplots(figsize=(17,10))\n","tidy = cv_res[['Algorithm', 'CVAccMeans', 'CVf1Means']].melt(id_vars='Algorithm').rename(columns=str.title)\n","sns.barplot(x='Algorithm', y='Value', hue='Variable', data=tidy, ax=ax, **{'yerr':[cv_acc_std, cv_f1_std]})\n","sns.despine(fig)\n","ax.set_xlabel(\"Algorithm\", size=14)\n","ax.set_ylabel(\"Score\", size=14)\n","#ax.legend(['Accuracy', 'F1 Score'])\n","ax.set_title(\"Cross validation scores\", size=16)\n","for item in ax.get_xticklabels():\n","    item.set_rotation(25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjliAFoBghX0"},"source":["def train_grid_search(estimator, param_grid, scoring, refit, cv=5, verbose=1, plot=True):\n","    model = GridSearchCV(estimator, param_grid=param_grid, cv=cv, scoring=scoring, verbose=verbose, \n","                         refit=refit, n_jobs=-1, return_train_score=True)\n","    model.fit(X_train, Y_train)\n","    \n","    results = model.cv_results_\n","    best_estimator = model.best_estimator_\n","    train_scores = results['mean_train_' + refit]\n","    test_scores = results['mean_test_' + refit]\n","    train_time = results['mean_fit_time']\n","    \n","    print(\"Best Score: \", model.best_score_)\n","    print(\"Best Param: \", model.best_params_)\n","    \n","    pred_train = best_estimator.predict(X_train)\n","    pred_test = best_estimator.predict(X_test)\n","\n","    acc, f1 = metric(Y_train, pred_train)\n","    logger.info('Training - acc: %.8f, f1: %.8f' % (acc, f1))\n","    acc, f1 = metric(Y_test, pred_test)\n","    logger.info('Test - acc: %.8f, f1: %.8f' % (acc, f1))\n","        \n","    if plot:\n","        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n","        fig.suptitle(\"GridSearchCV Result\", fontsize=20)\n","        \n","        ### First plot ###\n","        ax1.plot(train_scores, test_scores, 'bo')\n","        ax1.set_title(\"Train Score v.s. Test Score\", fontsize=16)\n","        ax1.set_xlabel(\"Train Score\")\n","        ax1.set_ylabel(\"Test Score\")\n","        ax1.set_xlim(0, 1)\n","        ax1.set_ylim(0, 1)\n","        ax1.grid(True)\n","        \n","        ### Second plot ###\n","        x_param = list(param_grid.keys())[0]\n","        x_param_min = np.min(list(param_grid.values())[0])\n","        x_param_max = np.max(list(param_grid.values())[0])\n","\n","        ax2.set_title(\"Score over the first param\", fontsize=16)\n","        ax2.set_xlabel(x_param)\n","        ax2.set_ylabel(\"Score\")\n","        ax2.set_xlim(x_param_min, x_param_max)\n","        ax2.set_ylim(0, 1)\n","\n","        # Get the regular numpy array from the MaskedArray\n","        X_axis = np.array(results['param_' + x_param].data, dtype=float)\n","\n","        for scorer, color in zip(sorted(scoring), ['r', 'g']):\n","            for sample, style in (('train', '--'), ('test', '-')):\n","                sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n","                sample_score_std = results['std_%s_%s' % (sample, scorer)]\n","                ax2.fill_between(X_axis, sample_score_mean - sample_score_std,\n","                                sample_score_mean + sample_score_std,\n","                                alpha=0.1 if sample == 'test' else 0, color=color)\n","                ax2.plot(X_axis, sample_score_mean, style, color=color,\n","                        alpha=1 if sample == 'test' else 0.7,\n","                        label=\"%s (%s)\" % (scorer, sample.capitalize()))\n","\n","            best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n","            best_score = results['mean_test_%s' % scorer][best_index]\n","\n","            # Plot a dotted vertical line at the best score for that scorer marked by x\n","            ax2.plot([X_axis[best_index], ] * 2, [0, best_score],\n","                    linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n","\n","            # Annotate the best score for that scorer\n","            ax2.annotate(\"%0.2f\" % best_score,\n","                        (X_axis[best_index], best_score + 0.005))\n","\n","        ax2.legend(loc=\"best\")\n","        ax2.grid(False)\n","        \n","        ### Third plot (Learning Curve) ###\n","        # Calculate learning curve (Accuracy)\n","        lc_acc_train_sizes, lc_acc_train_scores, lc_acc_test_scores = learning_curve(\n","            best_estimator, X_train, Y_train, cv=kfold, n_jobs=-1, scoring=scoring['Accuracy'], \n","            train_sizes=np.linspace(.1, 1.0, 5))\n","        lc_acc_train_mean = np.mean(lc_acc_train_scores, axis=1)\n","        lc_acc_train_std = np.std(lc_acc_train_scores, axis=1)\n","        lc_acc_test_mean = np.mean(lc_acc_test_scores, axis=1)\n","        lc_acc_test_std = np.std(lc_acc_test_scores, axis=1)\n","        \n","        # Calculate learning curve (F1 Score)\n","        lc_f1_train_sizes, lc_f1_train_scores, lc_f1_test_scores = learning_curve(\n","            best_estimator, X_train, Y_train, cv=kfold, n_jobs=-1, scoring=scoring['F1'], \n","            train_sizes=np.linspace(.1, 1.0, 5))\n","        lc_f1_train_mean = np.mean(lc_f1_train_scores, axis=1)\n","        lc_f1_train_std = np.std(lc_f1_train_scores, axis=1)\n","        lc_f1_test_mean = np.mean(lc_f1_test_scores, axis=1)\n","        lc_f1_test_std = np.std(lc_f1_test_scores, axis=1)\n","        \n","        ax3.set_title(\"Learning Curve\", fontsize=16)\n","        ax3.set_xlabel(\"Training examples\")\n","        ax3.set_ylabel(\"Score\")\n","\n","        # Plot learning curve (Accuracy)\n","        ax3.fill_between(lc_acc_train_sizes, \n","                         lc_acc_train_mean - lc_acc_train_std,\n","                         lc_acc_train_mean + lc_acc_train_std, alpha=0.1, color=\"r\")\n","        ax3.fill_between(lc_acc_train_sizes, \n","                         lc_acc_test_mean - lc_acc_test_std,\n","                         lc_acc_test_mean + lc_acc_test_std, alpha=0.1, color=\"r\")\n","        ax3.plot(lc_acc_train_sizes, lc_acc_train_mean, 'o--', color=\"r\",\n","                 label=\"Accuracy (Train)\")\n","        ax3.plot(lc_acc_train_sizes, lc_acc_test_mean, 'o-', color=\"r\",\n","                 label=\"Accuracy (Test)\")\n","        \n","        # Plot learning curve (F1 Score)\n","        ax3.fill_between(lc_f1_train_sizes, \n","                         lc_f1_train_mean - lc_f1_train_std,\n","                         lc_f1_train_mean + lc_f1_train_std, alpha=0.1, color=\"g\")\n","        ax3.fill_between(lc_f1_train_sizes, \n","                         lc_f1_test_mean - lc_f1_test_std,\n","                         lc_f1_test_mean + lc_f1_test_std, alpha=0.1, color=\"g\")\n","        ax3.plot(lc_f1_train_sizes, lc_f1_train_mean, 'o--', color=\"g\",\n","                 label=\"F1 (Train)\")\n","        ax3.plot(lc_f1_train_sizes, lc_f1_test_mean, 'o-', color=\"g\",\n","                 label=\"F1 (Test)\")\n","\n","        ax3.legend(loc=\"best\")\n","        ax3.grid(True)\n","        \n","        plt.tight_layout(pad=3.0)\n","        plt.show()\n","        \n","        ### Confusion Matrix ###\n","        class_names = ['Lower', 'Hold', 'Raise']\n","        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n","        fig.suptitle(\"Confusion Matrix\", fontsize=20)\n","        \n","        plot_confusion_matrix(best_estimator, X_train, Y_train, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize=None, ax=ax1)\n","        ax1.set_title(\"Train Data: Actual Count\")\n","        ax1.grid(False)\n","        \n","        plot_confusion_matrix(best_estimator, X_train, Y_train, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize='all', ax=ax2)\n","        ax2.set_title=(\"Train Data: Normalized\")\n","        ax2.grid(False)\n","        \n","        plot_confusion_matrix(best_estimator, X_test, Y_test, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize=None, ax=ax3)\n","        ax3.set_title=(\"Test Data: Actual Count\")\n","        ax3.grid(False)\n","        \n","        plot_confusion_matrix(best_estimator, X_test, Y_test, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize='all', ax=ax4)\n","        ax4.set_title(\"Test Data: Normalized\")\n","        ax4.grid(False)\n","        \n","        plt.tight_layout(pad=3.0)\n","        plt.show()\n","    \n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"23hNCpSZghX0"},"source":["# Define metrics\n","# Here, use F1 Macro to evaluate the model.\n","def metric(y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred, average='macro')\n","    return acc, f1\n","\n","scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n","refit = 'F1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6CmeuKAghX0"},"source":["X_train, X_test, Y_train, Y_test = \\\n","model_selection.train_test_split(X1, y1, test_size=0.2, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_li4pqnghX0"},"source":["# Define Classifier\n","DTC = DecisionTreeClassifier()\n","ada_clf = AdaBoostClassifier(DTC, random_state=rand_seed)\n","\n","#Perform Grid Search\n","param_grid = {'n_estimators': np.linspace(1, 100, 20, dtype=int),\n","              'base_estimator__criterion': ['gini'],\n","              'base_estimator__splitter': ['random'],\n","              'algorithm': ['SAMME'],\n","              'learning_rate': [0.0001]}\n","\n","ada_model = train_grid_search(ada_clf, param_grid, scoring, refit, cv=kfold, verbose=1, plot=True)\n","ada_best = ada_model.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9k_o1XnzghX0"},"source":["# Define Classifier\n","DTC = DecisionTreeClassifier()\n","ada_clf = AdaBoostClassifier(DTC, random_state=rand_seed)\n","#ExtraTrees \n","ext_clf = ExtraTreesClassifier()\n","# Perform Grid Search\n","param_grid = {'n_estimators': np.linspace(1, 200, 10, dtype=int),\n","              'min_samples_split': [4],\n","              'min_samples_leaf': [8],\n","              'max_features': [8],\n","              'max_depth': [None],\n","              'criterion': ['gini'],\n","              'bootstrap': [False]}\n","\n","ext_model = train_grid_search(ext_clf, param_grid, scoring, refit, cv=kfold, verbose=1, plot=True)\n","ext_best = ext_model.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMV7HMhoghX0"},"source":["# Random Forest\n","rf_clf = RandomForestClassifier()\n","\n","# Perform Grid Search\n","param_grid = {'n_estimators': np.linspace(1, 500, 10, dtype=int),\n","              'min_samples_split': [8],\n","              'min_samples_leaf': [8],\n","              'max_features': [8],\n","              'max_depth': [None],\n","              'criterion': ['gini'],\n","              'bootstrap': [False]}\n","\n","rf_model = train_grid_search(rf_clf, param_grid, scoring, refit, cv=kfold, verbose=1, plot=True)\n","rf_best = rf_model.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yl60p7cbghX0"},"source":["# Gradient boosting\n","gb_clf = GradientBoostingClassifier(random_state=2)\n","\n","# Perform Grid Search\n","param_grid = {'n_estimators': [1, 20, 50, 80, 130, 200, 300, 400],\n","              'min_samples_leaf': [16],\n","              'max_features': [0.8],\n","              'max_depth': [4],\n","              'loss': ['deviance'],\n","              'learning_rate': [0.05]}\n","\n","gb_model = train_grid_search(gb_clf, param_grid, scoring, refit, cv=kfold, verbose=1, plot=True)\n","gb_best = gb_model.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GT5twORUghX0"},"source":["nrows = ncols = 2\n","fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n","\n","names_classifiers = [(\"AdaBoosting\", ada_best), \n","                     (\"ExtraTrees\", ext_best), \n","                     (\"RandomForest\", rf_best), \n","                     (\"GradientBoosting\",gb_best)]\n","\n","nclassifier = 0\n","for row in range(nrows):\n","    for col in range(ncols):\n","        name = names_classifiers[nclassifier][0]\n","        classifier = names_classifiers[nclassifier][1]\n","        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n","        g = sns.barplot(y=train_df.columns[indices][:40], x=classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n","        g.set_xlabel(\"Relative importance\",fontsize=12)\n","        g.set_ylabel(\"Features\",fontsize=12)\n","        g.tick_params(labelsize=9)\n","        g.set_title(name + \" feature importance\")\n","        nclassifier += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qctGMLqdghX0"},"source":["nrows = ncols = 2\n","fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n","\n","names_classifiers = [(\"AdaBoosting\", ada_best), \n","                     (\"ExtraTrees\", ext_best), \n","                     (\"RandomForest\", rf_best), \n","                     (\"GradientBoosting\",gb_best)]\n","\n","nclassifier = 0\n","for row in range(nrows):\n","    for col in range(ncols):\n","        name = names_classifiers[nclassifier][0]\n","        classifier = names_classifiers[nclassifier][1]\n","        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n","        g = sns.barplot(y=train_df.columns[indices][:40], x=classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n","        g.set_xlabel(\"Relative importance\",fontsize=12)\n","        g.set_ylabel(\"Features\",fontsize=12)\n","        g.tick_params(labelsize=9)\n","        g.set_title(name + \" feature importance\")\n","        nclassifier += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXndPfTXghX0"},"source":["test_ada = pd.Series(ada_best.predict(X_test), name=\"Ada\")\n","test_ext = pd.Series(ext_best.predict(X_test), name=\"ExtC\")\n","test_rf = pd.Series(rf_best.predict(X_test), name=\"RFC\")\n","test_gb = pd.Series(gb_best.predict(X_test), name=\"GBC\")\n","\n","# Concatenate all classifier results\n","ensemble_results = pd.concat([test_ada, test_ext, test_rf, test_gb],axis=1)\n","\n","g = sns.heatmap(ensemble_results.corr(),annot=True, cmap=\"coolwarm\", center=0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWPRxIMQghX0"},"source":["# The same including all the other classifiers\n","test_resuts = []\n","\n","for classifier in classifiers:\n","    estimator = classifier[1].fit(X_train, Y_train)\n","    test_resuts.append(pd.Series(estimator.predict(X_test), name=classifier[0]))\n","\n","base_results = pd.concat(test_resuts, axis=1)\n","\n","plt.figure(figsize=(20,10))\n","g = sns.heatmap(base_results.corr(),annot=True, cmap=\"coolwarm\", center=0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JsAH3WPaghX0"},"source":["# Voting Classifier with \"soft\" to take all the probability into account\n","voting_best = VotingClassifier(estimators=[('adac', ada_best),\n","                                           ('extc', ext_best), \n","                                           ('rfc', rf_best), \n","                                           ('gbc', gb_best)], voting='soft', n_jobs=-1)\n","\n","# # Voting Classifier with \"soft\" to take all the probability into account\n","# voting_base = VotingClassifier(estimators=classifiers, voting='soft', n_jobs=4)\n","\n","voting_best.fit(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAmHnchEghX0"},"source":["# Predict\n","voting_pred_train = voting_best.predict(X_train)\n","voting_pred_test = voting_best.predict(X_test)\n","\n","acc, f1 = metric(Y_train, voting_pred_train)\n","logger.info('Train - acc: %.8f, f1: %.8f' % (acc, f1))\n","\n","acc, f1 = metric(Y_test, voting_pred_test)\n","logger.info('Test - acc: %.8f, f1: %.8f' % (acc, f1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GT-nWqALghX0"},"source":["X_test_s = pd.DataFrame(X_test)\n","X_test_stacked = pd.concat([X_test_s, X_test_s, X_test_s, X_test_s], axis=0)\n","pred_test_stacked = pd.concat([test_ada, test_ext, test_rf, test_gb],axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggfKy7eRghX0"},"source":["import xgboost as xgb\n","gbm = xgb.XGBClassifier(\n","    n_estimator=2000,\n","    max_depth=4,\n","    min_child_weight=2,\n","    gamma=0.9,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    objective='binary:logistic',\n","    nthread=-1,\n","    scale_pos_weight=1).fit(X_test_stacked, pred_test_stacked)\n","\n","# Predict\n","gbm_pred_train = gbm.predict(pd.DataFrame(X_train))\n","gbm_pred_test = gbm.predict(X_test_s)\n","\n","acc, f1 = metric(Y_train, gbm_pred_train)\n","logger.info('Train - acc: %.8f, f1: %.8f' % (acc, f1))\n","\n","acc, f1 = metric(Y_test, gbm_pred_test)\n","logger.info('Test - acc: %.8f, f1: %.8f' % (acc, f1))"],"execution_count":null,"outputs":[]}]}